{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5562,"status":"ok","timestamp":1763560839762,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"xOI2Cs_SZNya"},"outputs":[],"source":["!pip install -q unidecode"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1763560839776,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"g-L58ZG0iV4z"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","import unidecode\n","\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1763560839822,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"ECI1t16uZXeB","outputId":"f4f2a2c3-a508-44f0-e337-16b83fbc6b9f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":24}],"source":[" nltk.download('stopwords')"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1763560839828,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"8tKCySw5Zd-J"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79,"status":"ok","timestamp":1763560839909,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"OSc_mO9aiWmO","outputId":"52c9bea6-4105-47ef-e145-beef70bfca5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7e37142cf2f0>"]},"metadata":{},"execution_count":26}],"source":["seed = 1\n","torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83116,"status":"ok","timestamp":1763560922981,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"YZC0X-v9V82R","outputId":"6d3eeab9-95a1-44f3-f333-2352d0ce1648"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1uYXI4O3oWBA6QC8ZJ-r6yaTTfkdAnl_Q\n","To: /content/dataset.zip\n","100% 230k/230k [00:00<00:00, 4.01MB/s]\n","Archive:  /content/dataset.zip\n","replace dataset/all-data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: dataset/all-data.csv    \n"]}],"source":["!pip install -q gdown\n","!gdown --id 1uYXI4O3oWBA6QC8ZJ-r6yaTTfkdAnl_Q\n","!unzip /content/dataset.zip"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66,"status":"ok","timestamp":1763560923006,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"QIxc860OX-mC","outputId":"30a2d3c7-c6d2-4a09-df62-e1de96e40dbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["     sentiment                                            content\n","0      neutral  According to Gran , the company has no plans t...\n","1      neutral  Technopolis plans to develop in stages an area...\n","2     negative  The international electronic industry company ...\n","3     positive  With the new production plant the company woul...\n","4     positive  According to the company 's updated strategy f...\n","...        ...                                                ...\n","4841  negative  LONDON MarketWatch -- Share prices ended lower...\n","4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n","4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n","4844  negative  Net sales of the Paper segment decreased to EU...\n","4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n","\n","[4846 rows x 2 columns]\n"]}],"source":["data_path = '/content/dataset/all-data.csv'\n","headers = ['sentiment', 'content']\n","\n","df = pd.read_csv(data_path, names = headers, encoding = 'ISO-8859-1')\n","print(df)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1763560923008,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"KzAfFOQNe_YG","outputId":"8af67291-6c2f-473f-e576-3a08129f59f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'neutral': 0, 'negative': 1, 'positive': 2}\n","      sentiment                                            content\n","0             0  According to Gran , the company has no plans t...\n","1             0  Technopolis plans to develop in stages an area...\n","2             1  The international electronic industry company ...\n","3             2  With the new production plant the company woul...\n","4             2  According to the company 's updated strategy f...\n","...         ...                                                ...\n","4841          1  LONDON MarketWatch -- Share prices ended lower...\n","4842          0  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n","4843          1  Operating profit fell to EUR 35.4 mn from EUR ...\n","4844          1  Net sales of the Paper segment decreased to EU...\n","4845          1  Sales in Finland decreased by 10.5 % in Januar...\n","\n","[4846 rows x 2 columns]\n"]}],"source":["classes = {\n","    class_name: idx for idx, class_name in enumerate(df['sentiment'].unique())\n","}\n","print(classes)\n","df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])\n","print(df)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1763560923015,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"0abirE7zf1P0"},"outputs":[],"source":["# preprocessing\n","english_stop_words = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","def text_normalize(text):\n","  text = text.lower() #Lowercase\n","  text = unidecode.unidecode(text) # xoa dau cau PunctuationRemoval\n","  text = text.strip() # xoa khoang trang dau cuoi\n","  text = re.sub(r'[^\\w\\s]', '', text) # xoa khoang trang thua\n","\n","  text = ' '.join([word for word in text.split(' ') if word not in english_stop_words]) # StopwordsRemoval\n","  text = ' '.join([stemmer.stem(word) for word in text.split(' ')]) # Stemming (convert cac dang- thi cua tu ve origin word)\n","\n","  return text"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":3089,"status":"ok","timestamp":1763560926107,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"J-QTfw4Sh7yV"},"outputs":[],"source":["#build vocab\n","vocab = []\n","for sequence in df['content'].tolist():\n","  tokens = sequence.split()\n","  for token in tokens:\n","    if token not in vocab:\n","      vocab.append(token)\n","\n","vocab.append('UNK')\n","vocab.append('PAD')\n","\n","word_to_idx = {\n","    word: idx for idx, word in enumerate(vocab)\n","}"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1763560926136,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"KqONOTq_QL6h"},"outputs":[],"source":["# convert text token to token id\n","def transform(text, word_to_idx, max_seq_len):\n","  tokens = []\n","  for word in text.split():\n","    try:\n","      word_ids = word_to_idx[word]\n","    except:\n","      word_ids = word_to_idx['UNK']\n","    tokens.append(word_ids)\n","  if len(tokens) < max_seq_len:\n","    tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n","  elif len(tokens) > max_seq_len:\n","    tokens = tokens[:max_seq_len]\n","  return tokens"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1763560926168,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"IQt5mfIFeY66"},"outputs":[],"source":["val_size = 0.2\n","test_size = 0.125\n","is_shuffle = True\n","\n","texts = df['content'].to_list()\n","labels = df['sentiment'].to_list()\n","\n","x_train, x_val, y_train, y_val = train_test_split(texts, labels,\n","                                                  test_size = val_size,\n","                                                  shuffle = is_shuffle,\n","                                                  random_state= seed)\n","x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n","                                                    test_size = test_size,\n","                                                    shuffle = is_shuffle,\n","                                                    random_state= seed)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1763560926219,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"JjA9rCfwivjw"},"outputs":[],"source":["#build dataset\n","class SentimentDataset(Dataset):\n","  def __init__(self, x, y, word_to_idx, max_len_seq, transform = None):\n","    super().__init__()\n","    self.texts = x\n","    self.labels = y\n","    self.word_to_idx = word_to_idx\n","    self.max_len_seq = max_len_seq\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, idx):\n","    text = self.texts[idx]\n","    label = self.labels[idx]\n","\n","    if self.transform:\n","      text = self.transform(text, self.word_to_idx, self.max_len_seq)\n","    text = torch.tensor(text)\n","\n","    return text, label"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1763560926222,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"aPBYvYmPXgyZ"},"outputs":[],"source":["#dataloader\n","max_len_seq = 32\n","train_dataset = SentimentDataset(x_train, y_train, word_to_idx, max_len_seq, transform)\n","val_dataset = SentimentDataset(x_val, y_val, word_to_idx, max_len_seq, transform)\n","test_dataset = SentimentDataset(x_test, y_test, word_to_idx, max_len_seq, transform)\n","\n","train_batch_size = 128\n","test_batch_size = 8\n","train_dataloader = DataLoader(\n","    dataset= train_dataset,\n","    batch_size= train_batch_size,\n","    shuffle= True,\n","    drop_last= False\n",")\n","val_dataloader = DataLoader(\n","    dataset= val_dataset,\n","    batch_size= test_batch_size,\n","    shuffle= False,\n","    drop_last= False\n",")\n","test_dataloader = DataLoader(\n","    dataset= test_dataset,\n","    batch_size= test_batch_size,\n","    shuffle= False,\n","    drop_last= False\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1763560926275,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"j8yP0kxJPrkR"},"outputs":[],"source":["class Sentiment_RNN(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim,\n","               hidden_size, n_layers,\n","               n_classes, dropout_prob):\n","    super().__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.rnn = nn.RNN(\n","        input_size = embedding_dim,\n","        hidden_size = hidden_size,\n","        num_layers = n_layers,\n","        batch_first= True\n","    )\n","    self.norm = nn.LayerNorm(hidden_size)\n","    self.dropout = nn.Dropout(dropout_prob)\n","    self.fc1 = nn.Linear(hidden_size, 16)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(16, n_classes)\n","\n","  def forward(self, x):\n","    x = self.embedding(x)\n","    x, hn = self.rnn(x) # hn contains the final hidden state for each layer\n","    # x shape (Batch,Layer,Feature)\n","    x = x[:,-1,:] # take last layer\n","\n","    x = self.norm(x)\n","    x = self.dropout(x)\n","    x = self.fc1(x)\n","    x = self.relu(x)\n","    x = self.fc2(x)\n","\n","    return x"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1763560926323,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"G8Wv5HmqU-oT"},"outputs":[],"source":["vocab_size = len(vocab)\n","embedding_dim = 64\n","hidden_size = 64\n","n_layers = 2\n","n_classes = len(list(classes.keys()))\n","dropout_prob = 0.2\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#model\n","model = Sentiment_RNN(\n","  vocab_size= vocab_size,\n","  embedding_dim= embedding_dim,\n","  hidden_size= hidden_size,\n","  n_layers= n_layers,\n","  n_classes= n_classes,\n","  dropout_prob= dropout_prob).to(device)\n","\n","#setup loss func and optimizer\n","lr = 1e-4\n","epochs = 50\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1763561169596,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"sV879_BGR_q6"},"outputs":[],"source":["def train():\n","    train_losses = []\n","    val_losses = []\n","    for epoch in range(epochs):\n","      batch_train_losses = []\n","\n","      model.train()\n","      for idx, (texts, labels) in enumerate(train_dataloader):\n","        texts = texts.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(texts)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_train_losses.append(loss.item())\n","\n","      train_loss = sum(batch_train_losses) / len(batch_train_losses)\n","      train_losses.append(train_loss)\n","\n","      model.eval()\n","      all_predictions = []\n","      all_labels = []\n","      with torch.no_grad():\n","        batch_val_losses = []\n","        for idx, (texts, labels) in enumerate(val_dataloader):\n","          all_labels.extend(labels)\n","          texts = texts.to(device)\n","          labels = labels.to(device)\n","\n","          outputs = model(texts)\n","          indices = torch.argmax(outputs, dim = 1)\n","          all_predictions.extend(indices)\n","          loss = criterion(outputs, labels)\n","          batch_val_losses.append(loss.item())\n","\n","      val_loss = sum(batch_val_losses) / len(batch_val_losses)\n","      val_losses.append(val_loss)\n","\n","      all_labels = [label.item() for label in all_labels]\n","      all_predictions = [prediction.item() for prediction in all_predictions]\n","      metric = classification_report(all_labels, all_predictions)\n","      print(\"Epoch: {}/{},Train Loss: {:.4f}, Val Loss: {:.4f}\".format(epoch + 1, epochs, train_loss, val_loss))\n","      print(metric)\n",""]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14258,"status":"ok","timestamp":1763561185271,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"},"user_tz":-420},"id":"Jg7cfbeeXoGi","outputId":"9ef55655-f83c-4a64-ffbc-329ce7af681e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/50,Train Loss: 0.8931, Val Loss: 0.9265\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.96      0.74       570\n","           1       0.00      0.00      0.00       122\n","           2       0.41      0.09      0.14       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.34      0.35      0.29       970\n","weighted avg       0.47      0.59      0.47       970\n","\n","Epoch: 2/50,Train Loss: 0.8917, Val Loss: 0.9256\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.95      0.74       570\n","           1       0.00      0.00      0.00       122\n","           2       0.38      0.08      0.14       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.29       970\n","weighted avg       0.46      0.58      0.47       970\n","\n","Epoch: 3/50,Train Loss: 0.8884, Val Loss: 0.9259\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.95      0.74       570\n","           1       0.00      0.00      0.00       122\n","           2       0.42      0.10      0.16       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.34      0.35      0.30       970\n","weighted avg       0.48      0.59      0.48       970\n","\n","Epoch: 4/50,Train Loss: 0.8830, Val Loss: 0.9252\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.95      0.74       570\n","           1       0.00      0.00      0.00       122\n","           2       0.40      0.09      0.15       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.30       970\n","weighted avg       0.47      0.58      0.48       970\n","\n","Epoch: 5/50,Train Loss: 0.8852, Val Loss: 0.9265\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.37      0.10      0.16       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.32      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 6/50,Train Loss: 0.8791, Val Loss: 0.9269\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.38      0.09      0.15       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.29       970\n","weighted avg       0.46      0.58      0.47       970\n","\n","Epoch: 7/50,Train Loss: 0.8782, Val Loss: 0.9260\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.38      0.11      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 8/50,Train Loss: 0.8739, Val Loss: 0.9258\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.38      0.11      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 9/50,Train Loss: 0.8677, Val Loss: 0.9264\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.74       570\n","           1       0.00      0.00      0.00       122\n","           2       0.38      0.11      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.33      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 10/50,Train Loss: 0.8675, Val Loss: 0.9269\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.93      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.36      0.11      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.32      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 11/50,Train Loss: 0.8656, Val Loss: 0.9285\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.93      0.73       570\n","           1       0.00      0.00      0.00       122\n","           2       0.36      0.12      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.32      0.35      0.30       970\n","weighted avg       0.46      0.58      0.48       970\n","\n","Epoch: 12/50,Train Loss: 0.8612, Val Loss: 0.9288\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.14      0.01      0.02       122\n","           2       0.38      0.13      0.19       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.37      0.35      0.31       970\n","weighted avg       0.48      0.58      0.49       970\n","\n","Epoch: 13/50,Train Loss: 0.8595, Val Loss: 0.9319\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.14      0.01      0.02       122\n","           2       0.35      0.12      0.17       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.37      0.35      0.31       970\n","weighted avg       0.47      0.58      0.48       970\n","\n","Epoch: 14/50,Train Loss: 0.8566, Val Loss: 0.9298\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.14      0.01      0.02       122\n","           2       0.36      0.12      0.18       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.37      0.35      0.31       970\n","weighted avg       0.48      0.58      0.48       970\n","\n","Epoch: 15/50,Train Loss: 0.8529, Val Loss: 0.9323\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.12      0.01      0.02       122\n","           2       0.37      0.12      0.18       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.37      0.35      0.31       970\n","weighted avg       0.48      0.58      0.48       970\n","\n","Epoch: 16/50,Train Loss: 0.8467, Val Loss: 0.9366\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.22      0.02      0.03       122\n","           2       0.37      0.12      0.18       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.40      0.35      0.31       970\n","weighted avg       0.49      0.58      0.49       970\n","\n","Epoch: 17/50,Train Loss: 0.8426, Val Loss: 0.9319\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.30      0.02      0.05       122\n","           2       0.39      0.13      0.20       278\n","\n","    accuracy                           0.58       970\n","   macro avg       0.43      0.36      0.33       970\n","weighted avg       0.51      0.58      0.49       970\n","\n","Epoch: 18/50,Train Loss: 0.8391, Val Loss: 0.9341\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.92      0.73       570\n","           1       0.33      0.03      0.06       122\n","           2       0.41      0.14      0.21       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.45      0.36      0.33       970\n","weighted avg       0.52      0.59      0.50       970\n","\n","Epoch: 19/50,Train Loss: 0.8360, Val Loss: 0.9395\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.93      0.74       570\n","           1       0.33      0.03      0.06       122\n","           2       0.43      0.14      0.21       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.46      0.37      0.34       970\n","weighted avg       0.52      0.59      0.50       970\n","\n","Epoch: 20/50,Train Loss: 0.8280, Val Loss: 0.9322\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.93      0.74       570\n","           1       0.31      0.03      0.06       122\n","           2       0.43      0.14      0.21       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.45      0.37      0.34       970\n","weighted avg       0.52      0.59      0.50       970\n","\n","Epoch: 21/50,Train Loss: 0.8143, Val Loss: 0.9539\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.74      0.69       570\n","           1       0.27      0.03      0.06       122\n","           2       0.33      0.36      0.35       278\n","\n","    accuracy                           0.54       970\n","   macro avg       0.42      0.38      0.37       970\n","weighted avg       0.51      0.54      0.51       970\n","\n","Epoch: 22/50,Train Loss: 0.8316, Val Loss: 0.9241\n","              precision    recall  f1-score   support\n","\n","           0       0.62      0.91      0.73       570\n","           1       0.27      0.03      0.06       122\n","           2       0.42      0.17      0.24       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.43      0.37      0.34       970\n","weighted avg       0.52      0.59      0.51       970\n","\n","Epoch: 23/50,Train Loss: 0.8010, Val Loss: 0.9168\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.84      0.74       570\n","           1       0.28      0.04      0.07       122\n","           2       0.40      0.33      0.36       278\n","\n","    accuracy                           0.59       970\n","   macro avg       0.45      0.40      0.39       970\n","weighted avg       0.54      0.59      0.55       970\n","\n","Epoch: 24/50,Train Loss: 0.7878, Val Loss: 0.9062\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.90      0.75       570\n","           1       0.28      0.04      0.07       122\n","           2       0.43      0.26      0.33       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.45      0.40      0.38       970\n","weighted avg       0.54      0.61      0.55       970\n","\n","Epoch: 25/50,Train Loss: 0.7760, Val Loss: 0.9088\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.86      0.75       570\n","           1       0.26      0.04      0.07       122\n","           2       0.42      0.32      0.36       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.45      0.41      0.39       970\n","weighted avg       0.54      0.60      0.55       970\n","\n","Epoch: 26/50,Train Loss: 0.7715, Val Loss: 0.9047\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.91      0.76       570\n","           1       0.24      0.03      0.06       122\n","           2       0.44      0.24      0.31       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.44      0.40      0.38       970\n","weighted avg       0.54      0.61      0.54       970\n","\n","Epoch: 27/50,Train Loss: 0.7833, Val Loss: 0.8929\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.92      0.76       570\n","           1       0.25      0.03      0.06       122\n","           2       0.43      0.23      0.30       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.44      0.39      0.37       970\n","weighted avg       0.54      0.61      0.54       970\n","\n","Epoch: 28/50,Train Loss: 0.7513, Val Loss: 0.9042\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.85      0.75       570\n","           1       0.25      0.03      0.06       122\n","           2       0.41      0.33      0.37       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.44      0.40      0.39       970\n","weighted avg       0.54      0.60      0.55       970\n","\n","Epoch: 29/50,Train Loss: 0.7446, Val Loss: 0.8975\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.90      0.76       570\n","           1       0.22      0.03      0.06       122\n","           2       0.43      0.29      0.35       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.44      0.41      0.39       970\n","weighted avg       0.54      0.61      0.56       970\n","\n","Epoch: 30/50,Train Loss: 0.7426, Val Loss: 0.9056\n","              precision    recall  f1-score   support\n","\n","           0       0.64      0.93      0.76       570\n","           1       0.29      0.03      0.06       122\n","           2       0.40      0.19      0.26       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.44      0.38      0.36       970\n","weighted avg       0.53      0.60      0.53       970\n","\n","Epoch: 31/50,Train Loss: 0.7352, Val Loss: 0.9061\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.92      0.76       570\n","           1       0.31      0.03      0.06       122\n","           2       0.44      0.24      0.31       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.46      0.40      0.38       970\n","weighted avg       0.55      0.61      0.54       970\n","\n","Epoch: 32/50,Train Loss: 0.7310, Val Loss: 0.8918\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.87      0.76       570\n","           1       0.27      0.06      0.09       122\n","           2       0.41      0.30      0.35       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.45      0.41      0.40       970\n","weighted avg       0.55      0.61      0.56       970\n","\n","Epoch: 33/50,Train Loss: 0.7207, Val Loss: 0.8896\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.89      0.76       570\n","           1       0.32      0.07      0.12       122\n","           2       0.41      0.27      0.32       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.47      0.41      0.40       970\n","weighted avg       0.55      0.61      0.56       970\n","\n","Epoch: 34/50,Train Loss: 0.7159, Val Loss: 0.8929\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.86      0.76       570\n","           1       0.31      0.09      0.14       122\n","           2       0.40      0.31      0.35       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.46      0.42      0.41       970\n","weighted avg       0.55      0.60      0.56       970\n","\n","Epoch: 35/50,Train Loss: 0.7023, Val Loss: 0.9136\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.82      0.75       570\n","           1       0.29      0.11      0.16       122\n","           2       0.40      0.37      0.38       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.46      0.43      0.43       970\n","weighted avg       0.56      0.60      0.57       970\n","\n","Epoch: 36/50,Train Loss: 0.7025, Val Loss: 0.9031\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.83      0.75       570\n","           1       0.30      0.11      0.17       122\n","           2       0.39      0.33      0.36       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.46      0.43      0.43       970\n","weighted avg       0.56      0.60      0.57       970\n","\n","Epoch: 37/50,Train Loss: 0.6919, Val Loss: 0.9048\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.83      0.76       570\n","           1       0.31      0.14      0.19       122\n","           2       0.41      0.35      0.38       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.47      0.44      0.44       970\n","weighted avg       0.57      0.61      0.58       970\n","\n","Epoch: 38/50,Train Loss: 0.6819, Val Loss: 0.8940\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.89      0.77       570\n","           1       0.33      0.07      0.12       122\n","           2       0.43      0.30      0.35       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.48      0.42      0.41       970\n","weighted avg       0.56      0.62      0.57       970\n","\n","Epoch: 39/50,Train Loss: 0.7066, Val Loss: 0.9162\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.79      0.75       570\n","           1       0.29      0.17      0.22       122\n","           2       0.41      0.39      0.40       278\n","\n","    accuracy                           0.60       970\n","   macro avg       0.47      0.45      0.46       970\n","weighted avg       0.57      0.60      0.58       970\n","\n","Epoch: 40/50,Train Loss: 0.6820, Val Loss: 0.9033\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.84      0.76       570\n","           1       0.33      0.16      0.21       122\n","           2       0.44      0.37      0.40       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.49      0.45      0.46       970\n","weighted avg       0.58      0.62      0.59       970\n","\n","Epoch: 41/50,Train Loss: 0.6659, Val Loss: 0.8994\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.85      0.77       570\n","           1       0.32      0.10      0.15       122\n","           2       0.42      0.37      0.39       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.48      0.44      0.44       970\n","weighted avg       0.57      0.61      0.58       970\n","\n","Epoch: 42/50,Train Loss: 0.6522, Val Loss: 0.8984\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.88      0.77       570\n","           1       0.42      0.09      0.15       122\n","           2       0.43      0.32      0.37       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.51      0.43      0.43       970\n","weighted avg       0.58      0.62      0.57       970\n","\n","Epoch: 43/50,Train Loss: 0.6560, Val Loss: 0.9126\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.89      0.77       570\n","           1       0.41      0.11      0.18       122\n","           2       0.42      0.29      0.34       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.50      0.43      0.43       970\n","weighted avg       0.57      0.62      0.57       970\n","\n","Epoch: 44/50,Train Loss: 0.6448, Val Loss: 0.9030\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.84      0.77       570\n","           1       0.35      0.16      0.22       122\n","           2       0.43      0.37      0.40       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.50      0.46      0.46       970\n","weighted avg       0.58      0.62      0.59       970\n","\n","Epoch: 45/50,Train Loss: 0.6387, Val Loss: 0.9079\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.85      0.77       570\n","           1       0.33      0.11      0.17       122\n","           2       0.43      0.37      0.40       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.49      0.44      0.44       970\n","weighted avg       0.58      0.62      0.59       970\n","\n","Epoch: 46/50,Train Loss: 0.6251, Val Loss: 0.9152\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.81      0.76       570\n","           1       0.35      0.17      0.23       122\n","           2       0.43      0.40      0.41       278\n","\n","    accuracy                           0.61       970\n","   macro avg       0.50      0.46      0.47       970\n","weighted avg       0.58      0.61      0.59       970\n","\n","Epoch: 47/50,Train Loss: 0.6118, Val Loss: 0.9136\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.85      0.77       570\n","           1       0.35      0.11      0.17       122\n","           2       0.43      0.36      0.39       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.49      0.44      0.44       970\n","weighted avg       0.58      0.62      0.59       970\n","\n","Epoch: 48/50,Train Loss: 0.6183, Val Loss: 0.9101\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.88      0.77       570\n","           1       0.37      0.12      0.18       122\n","           2       0.43      0.31      0.36       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.49      0.44      0.44       970\n","weighted avg       0.57      0.62      0.58       970\n","\n","Epoch: 49/50,Train Loss: 0.6171, Val Loss: 0.9146\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.84      0.77       570\n","           1       0.33      0.15      0.20       122\n","           2       0.44      0.38      0.41       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.50      0.46      0.46       970\n","weighted avg       0.59      0.62      0.60       970\n","\n","Epoch: 50/50,Train Loss: 0.6159, Val Loss: 0.9288\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.91      0.77       570\n","           1       0.42      0.11      0.17       122\n","           2       0.43      0.26      0.32       278\n","\n","    accuracy                           0.62       970\n","   macro avg       0.51      0.42      0.42       970\n","weighted avg       0.57      0.62      0.57       970\n","\n"]}],"source":["train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"126bIYy7ZS0RLqWFHG8ppNYODJUssLeeN","authorship_tag":"ABX9TyNMy0M1PLt+6SKUShOUm27Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}