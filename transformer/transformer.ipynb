{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Kk3Q-s5i7qc1TsH4nfoXQsuGlVk6w7p8","authorship_tag":"ABX9TyMQ7CbZVPPv5PbWRFAdthqp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q datasets\n","!pip install -q torchtext==0.17.2"],"metadata":{"id":"bWxHQ0MwR_o6","executionInfo":{"status":"ok","timestamp":1765380375009,"user_tz":-420,"elapsed":10349,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.modules.activation import LeakyReLU\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from sklearn.metrics import accuracy_score\n","\n","import numpy as np\n","import math\n","import string\n","import re\n","import matplotlib.pyplot as plt\n","import os"],"metadata":{"id":"nseBmvjqfAI6","executionInfo":{"status":"ok","timestamp":1765380375019,"user_tz":-420,"elapsed":4,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["BUILD TRANSFORMER"],"metadata":{"id":"yxa0CYP7OCXh"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, max_len, device = 'cpu'):\n","    super().__init__()\n","    self.word_embedding = nn.Embedding(\n","        num_embeddings= vocab_size,\n","        embedding_dim = embedding_dim\n","    )\n","    self.position_embedding = nn.Embedding(\n","        num_embeddings= max_len,\n","        embedding_dim = embedding_dim\n","    )\n","\n","  def forward(self,x):\n","    N_samples_in_batch, seq_length = x.size()\n","    positions = torch.arange(0, seq_length, device=x.device).expand(N_samples_in_batch, seq_length)\n","    output1 = self.word_embedding(x)\n","    output2 = self.position_embedding(positions)\n","    return output1 + output2"],"metadata":{"id":"YoTuO-qagtjj","executionInfo":{"status":"ok","timestamp":1765380375026,"user_tz":-420,"elapsed":3,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Module):\n","  def __init__(self, embedding_dim, num_heads, ffn_dim, dropout = 0.1):\n","    super().__init__ ()\n","    self.attention = nn.MultiheadAttention(\n","        embed_dim= embedding_dim,\n","        num_heads= num_heads,\n","        batch_first= True  # B x Sqen_len x Em_dim\n","    )\n","    self.cross_attention = nn.MultiheadAttention(\n","        embed_dim= embedding_dim,\n","        num_heads= num_heads,\n","        batch_first= True\n","    )\n","    self.ffn = nn.Sequential(\n","        nn.Linear(in_features= embedding_dim, out_features= ffn_dim,bias= True),\n","        nn.LeakyReLU(),\n","        nn.Linear(in_features= ffn_dim, out_features= embedding_dim, bias= True)\n","    )\n","    self.layerNorm = nn.LayerNorm(normalized_shape= embedding_dim, eps= 1e-06)\n","    self.dropout = nn.Dropout(p= dropout)\n","\n","  def forward(self, q, k, v):\n","    attention_output, _ = self.attention(query= q, key= k, value= v)\n","    attention_output = self.dropout(attention_output)\n","\n","    output_1 = self.layerNorm(q + attention_output) # attention output + q (skip connection)\n","\n","    ffn_output = self.ffn(output_1)\n","    ffn_output = self.dropout(ffn_output)\n","\n","    output_2 = self.layerNorm(output_1 + ffn_output) # ffn output + output_1 (skip connection)\n","\n","    return output_2"],"metadata":{"id":"1ZtcYEFTKloc","executionInfo":{"status":"ok","timestamp":1765380375032,"user_tz":-420,"elapsed":2,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, max_length, num_layers, num_heads, ffn_dim, dropout = 0.1, device = 'cpu'):\n","    super().__init__()\n","    self.embedding = TokenAndPositionEmbedding(\n","        vocab_size= vocab_size,\n","        embedding_dim= embedding_dim,\n","        max_len= max_length\n","    )\n","    self.layers = nn.ModuleList(\n","        [\n","            TransformerEncoderBlock(\n","                embedding_dim= embedding_dim,\n","                num_heads= num_heads,\n","                ffn_dim= ffn_dim,\n","                dropout= dropout\n","            ) for _ in range(num_layers)\n","        ]\n","    )\n","\n","  def forward(self,x):\n","    output = self.embedding(x)\n","    for layer in self.layers:\n","      output = layer(output,output,output)\n","\n","    return output"],"metadata":{"id":"jH9so1v4p_FY","executionInfo":{"status":"ok","timestamp":1765380375039,"user_tz":-420,"elapsed":3,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["#test\n","batch_size = 32\n","vocal_size = 1000\n","embedding_dim = 200\n","max_length = 100\n","num_layers = 3\n","num_heads = 4\n","ffn_dim = 1028\n","\n","input = torch.randint(\n","    high= 2,\n","    size= (batch_size, max_length), #32x100\n","    dtype= torch.int64\n",")\n"],"metadata":{"id":"hQjFKJr7tt4-","executionInfo":{"status":"ok","timestamp":1765380375046,"user_tz":-420,"elapsed":4,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["encoder = TransformerEncoder(\n","    vocab_size= vocal_size,\n","    embedding_dim= embedding_dim,\n","    max_length= max_length,\n","    num_layers= num_layers,\n","    num_heads= num_heads,\n","    ffn_dim= ffn_dim\n",")\n","\n","encoded = encoder(input)\n","encoded.shape"],"metadata":{"id":"xiCKRWlODqSN","executionInfo":{"status":"ok","timestamp":1765380375597,"user_tz":-420,"elapsed":553,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c36a8f79-b8cf-46db-eb8a-7134462f3936"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100, 200])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["class TransformerDecoderBlock(nn.Module):\n","  def __init__(self, embedding_dim, num_heads, ffn_dim, dropout= 0.1):\n","    super().__init__()\n","    self.attention = nn.MultiheadAttention(\n","        embed_dim= embedding_dim,\n","        num_heads= num_heads,\n","        batch_first= True\n","    )\n","    self.cross_attention = nn.MultiheadAttention(\n","        embed_dim= embedding_dim,\n","        num_heads= num_heads,\n","        batch_first= True\n","    )\n","    self.ffn = nn.Sequential(\n","        nn.Linear(\n","            in_features=embedding_dim,\n","            out_features= ffn_dim,\n","            bias= True\n","            ),\n","        nn.LeakyReLU(),\n","        nn.Linear(\n","            in_features=ffn_dim,\n","            out_features= embedding_dim,\n","            bias= True\n","            ),\n","    )\n","    self.layerNorm = nn.LayerNorm(normalized_shape= embedding_dim, eps= 1e-06)\n","    self.dropout = nn.Dropout(p= dropout)\n","\n","  def forward(self, x, encoder_output, src_mask, tgt_mask):\n","    attention_output, _ = self.attention(query= x, key= x, value= x, attn_mask= tgt_mask)\n","    attention_output = self.dropout(attention_output)\n","    output_1 = self.layerNorm(x + attention_output)\n","\n","    cross_attention_output, _ = self.cross_attention(query= output_1, key= encoder_output, value= encoder_output, attn_mask= src_mask)\n","    cross_attention_output = self.dropout(cross_attention_output)\n","    output_2 = self.layerNorm(output_1 + cross_attention_output)\n","\n","    ffn_output = self.ffn(output_2)\n","    ffn_output = self.dropout(ffn_output)\n","\n","    output_3 = self.layerNorm(output_2 + ffn_output)\n","\n","    return output_3"],"metadata":{"id":"gAc4MVx53ZUl","executionInfo":{"status":"ok","timestamp":1765380375616,"user_tz":-420,"elapsed":14,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","  def __init__(self, vocal_size, embedding_dim, max_length, num_layers, num_heads, ffn_dim, dropout = 0.1, device = 'cpu'):\n","    super().__init__()\n","    self.embedding = TokenAndPositionEmbedding(\n","        vocab_size= vocal_size,\n","        embedding_dim= embedding_dim,\n","        max_len= max_length,\n","        device= device\n","    )\n","    self.layers = nn.ModuleList([\n","        TransformerDecoderBlock(\n","            embedding_dim= embedding_dim,\n","            num_heads= num_heads,\n","            ffn_dim= ffn_dim,\n","            dropout= dropout\n","        ) for _ in range(num_layers)\n","    ])\n","  def forward(self, x, encoder_output, src_mask, tgt_mask):\n","    output = self.embedding(x)\n","    for layer in self.layers:\n","      output = layer(output, encoder_output, src_mask, tgt_mask)\n","\n","    return output"],"metadata":{"id":"4R1eHAuOBaXl","executionInfo":{"status":"ok","timestamp":1765380375633,"user_tz":-420,"elapsed":14,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","  def __init__(self, src_vocab_size, tgt_vocab_size,\n","               embedding_dim, max_length, num_layers, num_heads, ffn_dim,\n","               dropout = 0.1, device = 'cpu'):\n","    super().__init__()\n","    self.device = device\n","    self.encoder = TransformerEncoder(\n","        vocab_size= src_vocab_size,\n","        embedding_dim= embedding_dim,\n","        max_length= max_length,\n","        num_layers= num_layers,\n","        num_heads= num_heads,\n","        ffn_dim= ffn_dim,\n","        dropout= dropout,\n","        device= device\n","    )\n","    self.decoder = TransformerDecoder(\n","        vocal_size= tgt_vocab_size,\n","        embedding_dim= embedding_dim,\n","        max_length= max_length,\n","        num_layers= num_layers,\n","        num_heads= num_heads,\n","        ffn_dim= ffn_dim,\n","        dropout= dropout,\n","        device= device\n","    )\n","    self.fc = nn.Linear(in_features= embedding_dim, out_features= tgt_vocab_size)\n","\n","  def generate_mask(self, src, tgt):\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","\n","    src_mask = torch.zeros(\n","        size= (src_seq_len, src_seq_len),\n","        device= self.device\n","    ).type(torch.bool)\n","\n","    tgt_mask = (\n","        torch.triu(torch.ones(size= (tgt_seq_len, tgt_seq_len),device= self.device)) == 1\n","        ).transpose(0,1)\n","    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n","\n","    return src_mask, tgt_mask\n","\n","  def forward(self,src, tgt):\n","    src_mask, tgt_mask = self.generate_mask(src, tgt)\n","    encoder_output = self.encoder(src)\n","    decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","    output = self.fc(decoder_output)\n","\n","    return output"],"metadata":{"id":"zhmqccZ5H_pn","executionInfo":{"status":"ok","timestamp":1765380375674,"user_tz":-420,"elapsed":17,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["src_seq_len = 3\n","src_mask = torch.zeros(\n","        size= (src_seq_len, src_seq_len),\n","    ).type(torch.bool)\n","src_mask"],"metadata":{"id":"PgtYIvR2HslQ","executionInfo":{"status":"ok","timestamp":1765380375703,"user_tz":-420,"elapsed":22,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"16ae0307-6704-45b9-d140-394b672f2a0e"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False, False, False],\n","        [False, False, False],\n","        [False, False, False]])"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["tgt_seq_len = 3\n","tgt_mask = torch.ones(size= (tgt_seq_len, tgt_seq_len))\n","print(tgt_mask)\n","\n","tgt_mask = torch.triu(tgt_mask)\n","print(tgt_mask)\n","\n","tgt_mask = tgt_mask == 1\n","print(tgt_mask)\n","\n","tgt_mask = tgt_mask.transpose(0,1)\n","print(tgt_mask)\n"],"metadata":{"id":"8LwZXXjwH8MQ","executionInfo":{"status":"ok","timestamp":1765380375760,"user_tz":-420,"elapsed":53,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"37c56a98-b98f-438e-c52a-dd061ef161c8"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([[1., 1., 1.],\n","        [0., 1., 1.],\n","        [0., 0., 1.]])\n","tensor([[ True,  True,  True],\n","        [False,  True,  True],\n","        [False, False,  True]])\n","tensor([[ True, False, False],\n","        [ True,  True, False],\n","        [ True,  True,  True]])\n"]}]},{"cell_type":"code","source":["#test\n","batch_size = 128\n","src_vocab_size = 1000\n","tgt_vocab_size = 2000\n","embed_dim = 200\n","max_length = 100\n","num_layers = 2\n","num_heads = 4\n","ff_dim = 256\n","\n","model = Transformer(\n","    src_vocab_size , tgt_vocab_size ,\n","    embed_dim , max_length , num_layers , num_heads , ff_dim\n",")\n","\n","src = torch.randint(\n","    high =2,\n","    size =(batch_size , max_length),\n","    dtype = torch.int64\n",")\n","\n","tgt = torch.randint(\n","    high =2,\n","    size =(batch_size, max_length),\n","    dtype = torch.int64\n","    )\n","prediction = model (src, tgt)\n","prediction.shape # batch_size x max_length x tgt_vocab_size"],"metadata":{"id":"1VvyIns9JZI_","executionInfo":{"status":"ok","timestamp":1765380378292,"user_tz":-420,"elapsed":2536,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"088466ae-640b-4c13-8d27-594ce506f08b"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([128, 100, 2000])"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["TEST WITH TEXT CLASSIFICATION"],"metadata":{"id":"jVy3tKG_OGfY"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"thainq107/ntc-scv\")\n","ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r65o9BFyOKbw","outputId":"5c7d9a66-63c4-4bff-cd25-40e676474ced","executionInfo":{"status":"ok","timestamp":1765380379221,"user_tz":-420,"elapsed":911,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'preprocessed_sentence'],\n","        num_rows: 30000\n","    })\n","    valid: Dataset({\n","        features: ['sentence', 'label', 'preprocessed_sentence'],\n","        num_rows: 10000\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'preprocessed_sentence'],\n","        num_rows: 10000\n","    })\n","})"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["#preprocessing\n","def preprocessing_text(text):\n","  #remove url web\n","  url_pattern = re.compile(r'https ?://\\ s+\\ wwww \\.\\s+')\n","  text = url_pattern.sub(r\" \", text)\n","\n","  #remove html tag\n","  html_pattern = re.compile(r'<[^ < >]+ >')\n","  text = html_pattern.sub(\" \", text)\n","\n","  #remove punctuations and digits\n","  replace_chars = list(string.punctuation + string.digits)\n","  for char in replace_chars:\n","    text = text.replace(char, \" \")\n","\n","  #remove emoji\n","  emoji_pattern = re.compile(\"[\"\n","      u\"\\U0001F600-\\U0001F64F\" # emoticons\n","      u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n","      u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n","      u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n","      u\"\\U0001F1F2-\\U0001F1F4\" # Macau flag\n","      u\"\\U0001F1E6-\\U0001F1FF\" # flags\n","      u\"\\U0001F600-\\U0001F64F\"\n","      u\"\\U00002702-\\U000027B0\"\n","      u\"\\U000024C2-\\U0001F251\"\n","      u\"\\U0001f926-\\U0001f937\"\n","      u\"\\U0001F1F2\"\n","      u\"\\U0001F1F4\"\n","      u\"\\U0001F620\"\n","      u\"\\u200d\"\n","      u\"\\u2640-\\u2642\"\n","      \"]+\", flags = re.UNICODE\n","  )\n","  text = emoji_pattern.sub(r\" \", text)\n","\n","  #normalize whitespace\n","  text = \" \".join(text.split())\n","\n","  #lowercase\n","  text = text.lower()\n","\n","  return text"],"metadata":{"id":"crtyZmtRJux1","executionInfo":{"status":"ok","timestamp":1765380379238,"user_tz":-420,"elapsed":6,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def yield_tokens(sentences, tokenizer):\n","  for sentence in sentences:\n","    yield tokenizer(sentence)\n","\n","#tokenizer\n","tokenizer = get_tokenizer(\"basic_english\")\n","\n","#build vocab\n","vocab_size = 10000\n","vocab = build_vocab_from_iterator(\n","    yield_tokens(ds['train']['preprocessed_sentence'], tokenizer),\n","    specials = ['<pad>','<unk>'],\n","    max_tokens = vocab_size\n",")\n","\n","vocab.set_default_index(vocab['<unk>'])\n","\n","#convert dataset\n","def prepare_dataset(data):\n","  #create iter (sentence, label)\n","  for row in data:\n","    sentence = row[\"preprocessed_sentence\"]\n","    encoded_senctence = vocab(tokenizer(sentence))\n","    label = row[\"label\"]\n","    yield encoded_senctence, label\n"],"metadata":{"id":"lSv7mFMtVhBn","executionInfo":{"status":"ok","timestamp":1765380389873,"user_tz":-420,"elapsed":10640,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["#@title Thiết lập tham số huấn luyện\n","num_epochs = 50  #@param {type:\"integer\"}\n","batch_size = 128  #@param {type:\"integer\"}\n","learning_rate = 5e-5  #@param {type:\"number\"}\n","vocab_size = 10000  #@param {type:\"integer\"}\n","max_length = 100  #@param {type:\"integer\"}\n","embed_dim = 200  #@param {type:\"integer\"}\n","num_layers = 2  #@param {type:\"integer\"}\n","num_heads = 4  #@param {type:\"integer\"}\n","ffn_dim = 128  #@param {type:\"integer\"}\n","dropout = 0.1  #@param {type:\"number\"}\n","log_folder = \"tensorboard\"  #@param {type:\"string\"}\n","checkpoint_folder = \"checkpoint\"  #@param {type:\"string\"}\n","checkpoint = \"\"  #@param {type:\"string\"}\n","\n","from argparse import Namespace\n","def get_args():\n","  ckpt_path = checkpoint if checkpoint != \"\" else None\n","  args = Namespace(\n","      num_epochs=num_epochs,\n","      batch_size=batch_size,\n","      learning_rate = learning_rate,\n","      vocab_size=vocab_size,\n","      max_length=max_length,\n","      embed_dim=embed_dim,\n","      num_layers=num_layers,\n","      num_heads=num_heads,\n","      ffn_dim=ffn_dim,\n","      dropout=dropout,\n","      log_folder=log_folder,\n","      checkpoint_folder=checkpoint_folder,\n","      checkpoint = ckpt_path\n","    )\n","  return args"],"metadata":{"id":"WqtA9pL4aGiP","executionInfo":{"status":"ok","timestamp":1765380508607,"user_tz":-420,"elapsed":4,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["#dataloader\n","seq_length = 100\n","\n","def collate_fn(batch):\n","  sentences, labels = zip(*batch)\n","  encoded_sentences = [\n","      sentence + ([0] * (seq_length - len(sentence))) if len(sentence) < seq_length # [0] idx cua <pad>\n","      else sentence[:seq_length]\n","      for sentence in sentences\n","  ]\n","\n","  encoded_sentences = torch.tensor(encoded_sentences, dtype= torch.int64)\n","  labels = torch.tensor(labels)\n","\n","  return encoded_sentences, labels\n"],"metadata":{"id":"d4gbIY08PFhm","executionInfo":{"status":"ok","timestamp":1765380510719,"user_tz":-420,"elapsed":2,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["class Transformer_Encoder_Cls(nn.Module):\n","  def __init__(self, vocab_size, max_length, num_layers,\n","               embedding_dim, num_heads, ffn_dim,\n","               dropout = 0.1, device = 'cpu'):\n","    super().__init__()\n","    self.device = device\n","    self.encoder = TransformerEncoder(\n","        vocab_size= vocab_size,\n","        embedding_dim= embedding_dim,\n","        max_length= max_length,\n","        num_layers= num_layers,\n","        num_heads= num_heads,\n","        ffn_dim= ffn_dim,\n","        dropout= dropout,\n","        device= device\n","    )\n","\n","    self.fc1 = nn.Linear(in_features= embedding_dim, out_features= 32)\n","    self.fc2 = nn.Linear(in_features= 32, out_features= 2)\n","\n","    self.dropout = nn.Dropout(p= dropout)\n","    self.LeakyReLU = nn.LeakyReLU()\n","\n","  def forward(self, x):\n","    output = self.encoder(x)\n","    output = output.mean(dim= 1)\n","    output = self.fc1(output)\n","    output = self.LeakyReLU(output)\n","    output = self.dropout(output)\n","    output = self.fc2(output)\n","\n","    return output"],"metadata":{"id":"Yl076bBFY3N3","executionInfo":{"status":"ok","timestamp":1765380513981,"user_tz":-420,"elapsed":3,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["def train(args, ds):\n","    batch_size = args.batch_size\n","    num_epochs = args.num_epochs\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    save_folder = args.checkpoint_folder\n","    if not os.path.exists(save_folder):\n","        os.makedirs(save_folder)\n","        print(f\"Created folder: {save_folder}\")\n","\n","    train_dataset = prepare_dataset(ds['train'])\n","    train_dataset = to_map_style_dataset(train_dataset)\n","\n","    val_dataset = prepare_dataset(ds['valid'])\n","    val_dataset = to_map_style_dataset(val_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n","    )\n","\n","    val_dataloader = DataLoader(\n","        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n","    )\n","\n","    model = Transformer_Encoder_Cls(\n","        vocab_size=args.vocab_size,\n","        max_length=args.max_length,\n","        num_layers=args.num_layers,\n","        embedding_dim=args.embed_dim,\n","        num_heads=args.num_heads,\n","        ffn_dim=args.ffn_dim,\n","        dropout=args.dropout\n","    ).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optim = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n","    writer = SummaryWriter(log_dir=args.log_folder)\n","\n","    start_epoch = 0\n","    best_acc = 0.0\n","\n","    if args.checkpoint:\n","        if os.path.isfile(args.checkpoint):\n","            print(f\"Loading checkpoint '{args.checkpoint}'...\")\n","            checkpoint = torch.load(args.checkpoint)\n","\n","            model.load_state_dict(checkpoint['model'])\n","            optim.load_state_dict(checkpoint['optimizer'])\n","\n","            start_epoch = checkpoint['epoch']\n","            best_acc = checkpoint['best_acc']\n","\n","            print(f\"Resumed from epoch {start_epoch} with best_acc {best_acc:.4f}\")\n","        else:\n","            print(f\"Checkpoint not found at '{args.checkpoint}'. Training from scratch.\")\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        train_loss = 0\n","        model.train()\n","\n","        for iter, (sentences, labels) in enumerate(train_dataloader):\n","            sentences = sentences.to(device)\n","            labels = labels.to(device)\n","\n","            optim.zero_grad()\n","            outputs = model(sentences)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optim.step()\n","\n","            train_loss += loss.item()\n","\n","            current_step = epoch * len(train_dataloader) + iter\n","            writer.add_scalar(\"Loss/train_step\", loss.item(), current_step)\n","\n","        avg_train_loss = train_loss / len(train_dataloader)\n","        writer.add_scalar(\"Loss_avg/train\", avg_train_loss, epoch)\n","        print(f\"Train epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss:.4f}\")\n","\n","        model.eval()\n","        all_predicts = []\n","        all_labels = []\n","        val_loss = 0\n","\n","        with torch.no_grad():\n","            for sentences, labels in val_dataloader:\n","                sentences = sentences.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(sentences)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                _, predict_indices = torch.max(outputs, dim=1)\n","\n","                all_predicts.extend(predict_indices)\n","                all_labels.extend(labels)\n","\n","        all_labels = [label.item() for label in all_labels]\n","        all_predicts = [predict.item() for predict in all_predicts]\n","\n","        avg_val_loss = val_loss / len(val_dataloader)\n","        acc = accuracy_score(all_labels, all_predicts)\n","\n","        writer.add_scalar(\"Loss_avg/valid\", avg_val_loss, epoch)\n","        writer.add_scalar(\"Accuracy/valid\", acc, epoch)\n","\n","        print(f\"Val epoch {epoch + 1}/{num_epochs}, Loss: {avg_val_loss:.4f}, Acc: {acc:.4f}\")\n","\n","        is_best = False\n","        if acc > best_acc:\n","            best_acc = acc\n","            is_best = True\n","\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'model': model.state_dict(),\n","            'optimizer': optim.state_dict(),\n","            'best_acc': best_acc\n","        }\n","\n","        last_path = os.path.join(save_folder, 'last_cp.pt')\n","        torch.save(checkpoint, last_path)\n","\n","        if is_best:\n","            best_path = os.path.join(save_folder, 'best_cp.pt')\n","            torch.save(checkpoint, best_path)\n","            print(f\"--> Saved new best model (Acc: {best_acc:.4f})\")\n","\n","    writer.close()\n","    print(\"Complete\")"],"metadata":{"id":"YDJMnbJ4OJW8","executionInfo":{"status":"ok","timestamp":1765380739236,"user_tz":-420,"elapsed":45,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["args = get_args()\n","train(args,ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"2g6jTtPmg6tz","executionInfo":{"status":"error","timestamp":1765380844661,"user_tz":-420,"elapsed":717,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}},"outputId":"75b3b3cd-a516-4a47-e683-7dfcf5177dfe"},"execution_count":63,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3971797396.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1990517475.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, ds)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_map_style_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchtext/data/functional.py\u001b[0m in \u001b[0;36mto_map_style_dataset\u001b[0;34m(iter_data)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_MapStyleDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchtext/data/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iter_data)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;31m# TODO Avoid list issue #1296\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1512485064.py\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preprocessed_sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mencoded_senctence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mencoded_senctence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir /content/tensorboard"],"metadata":{"id":"DoSAZWQmy5TA","executionInfo":{"status":"aborted","timestamp":1765380389965,"user_tz":-420,"elapsed":25504,"user":{"displayName":"Noob Bro","userId":"10635442345836347201"}}},"execution_count":null,"outputs":[]}]}